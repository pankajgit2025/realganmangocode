{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12141342,"sourceType":"datasetVersion","datasetId":7646561},{"sourceId":12143531,"sourceType":"datasetVersion","datasetId":7648118}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!rm -rf /kaggle/working/*","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#WITH 512X512 IMAGE \nimport os\nimport torch\nimport shutil\nfrom torch import nn\nfrom torchvision import transforms\nfrom torchvision.utils import save_image\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom diffusers import UNet2DModel, DDPMScheduler, AutoencoderKL\nfrom accelerate import Accelerator\n\n# ======================\n# Dataset Loader\n# ======================\nclass MangoLeafDataset(Dataset):\n    def __init__(self, folder):\n        self.image_paths = [os.path.join(folder, f) for f in os.listdir(folder)\n                            if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n        self.transform = transforms.Compose([\n            transforms.Resize((512, 512)),\n            transforms.ToTensor(),\n            transforms.Normalize([0.5]*3, [0.5]*3)\n        ])\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img = Image.open(self.image_paths[idx]).convert(\"RGB\")\n        return self.transform(img)\n\n# ======================\n# Paths\n# ======================\nclass_name = \"ANTHRACNOSE\"\ndata_dir = f\"/kaggle/input/gananthracnose/{class_name}\"\noutput_dir = f\"/kaggle/working/DiffusionModel_{class_name.replace(' ', '_')}\"\nos.makedirs(output_dir, exist_ok=True)\n\n# ======================\n# Load VAE\n# ======================\nvae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\").to(\"cuda\").eval()\n\n# ======================\n# Model\n# ======================\ndef get_unet():\n    return UNet2DModel(\n        sample_size=64,   # latent size for 512x512 VAE (512/8)\n        in_channels=4,\n        out_channels=4,\n        layers_per_block=2,\n        block_out_channels=(128, 256, 512, 512),\n        down_block_types=(\"DownBlock2D\", \"DownBlock2D\", \"DownBlock2D\", \"AttnDownBlock2D\"),\n        up_block_types=(\"AttnUpBlock2D\", \"UpBlock2D\", \"UpBlock2D\", \"UpBlock2D\")\n    )\n\n# ======================\n# Dataset & DataLoader\n# ======================\ndataset = MangoLeafDataset(data_dir)\ndataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n\n# ======================\n# Training\n# ======================\nmodel = get_unet()\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\nscheduler = DDPMScheduler(num_train_timesteps=1000)\naccelerator = Accelerator()\nmodel, optimizer, dataloader = accelerator.prepare(model, optimizer, dataloader)\n\n# Training loop\nnum_epochs = 100\nmodel.train()\n\nfor epoch in range(1, num_epochs + 1):\n    print(f\"\\U0001f9ea Epoch {epoch}/{num_epochs}\")\n    for batch in tqdm(dataloader, desc=f\"Epoch {epoch}\"):\n        # Encode to latent space\n        with torch.no_grad():\n            latents = vae.encode(batch.to(accelerator.device)).latent_dist.sample() * 0.18215\n\n        noise = torch.randn_like(latents)\n        timesteps = torch.randint(0, scheduler.config.num_train_timesteps, (latents.size(0),), device=latents.device).long()\n        noisy_latents = scheduler.add_noise(latents, noise, timesteps)\n\n        noise_pred = model(noisy_latents, timesteps).sample\n        loss = nn.functional.mse_loss(noise_pred, noise)\n\n        optimizer.zero_grad()\n        accelerator.backward(loss)\n        optimizer.step()\n\n    # Save checkpoint every 10 epochs\n    if epoch % 10 == 0:\n        ckpt_dir = os.path.join(output_dir, f\"checkpoint_epoch_{epoch}\")\n        model.save_pretrained(ckpt_dir)\n        print(f\"\\U0001f4be Saved checkpoint at: {ckpt_dir}\")\n\n    # Save sample image every 20 epochs\n    if epoch % 20 == 0:\n        model.eval()\n        with torch.no_grad():\n            sample_latents = torch.randn(1, 4, 64, 64).to(accelerator.device)\n            for t in scheduler.timesteps:\n                noise_pred = model(sample_latents, t).sample\n                sample_latents = scheduler.step(noise_pred, t, sample_latents).prev_sample\n\n            decoded_img = vae.decode(sample_latents / 0.18215).sample\n            save_path = os.path.join(output_dir, f\"sample_epoch_{epoch}.png\")\n            save_image(decoded_img, save_path, normalize=True)\n            print(f\"\\U0001f5bcÔ∏è Saved sample image at: {save_path}\")\n        model.train()\n\n# ======================\n# Zip Final Model\n# ======================\nzip_path = shutil.make_archive(output_dir, 'zip', output_dir)\nprint(f\"‚úÖ Training complete. Zipped model at: {zip_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T09:59:54.982908Z","iopub.execute_input":"2025-06-12T09:59:54.983500Z","execution_failed":"2025-06-12T13:35:49.337Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"2025-06-12 10:00:06.946747: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749722407.143638      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749722407.199314      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/547 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32543cb97c3548fdaf3cdf84bad2357c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"diffusion_pytorch_model.safetensors:   0%|          | 0.00/335M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03fce2fbf0d84b529cebf6b981b3e3dc"}},"metadata":{}},{"name":"stdout","text":"üß™ Epoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2500/2500 [26:47<00:00,  1.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"üß™ Epoch 2/100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2500/2500 [24:05<00:00,  1.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"üß™ Epoch 3/100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2500/2500 [24:04<00:00,  1.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"üß™ Epoch 4/100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2500/2500 [24:06<00:00,  1.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"üß™ Epoch 5/100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2500/2500 [24:06<00:00,  1.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"üß™ Epoch 6/100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2500/2500 [24:09<00:00,  1.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"üß™ Epoch 7/100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2500/2500 [24:08<00:00,  1.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"üß™ Epoch 8/100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2500/2500 [24:11<00:00,  1.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"üß™ Epoch 9/100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2041/2500 [19:44<04:26,  1.72it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"#MULTI GPU CODE 256X256 LATENT DEFFUSION MODEL\nimport os\nimport torch\nimport shutil\nfrom torch import nn\nfrom torchvision import transforms\nfrom torchvision.utils import save_image\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom diffusers import UNet2DModel, DDPMScheduler, AutoencoderKL\nfrom accelerate import Accelerator\n\n# ======================\n# Dataset Loader\n# ======================\nclass MangoLeafDataset(Dataset):\n    def __init__(self, folder):\n        self.image_paths = [os.path.join(folder, f) for f in os.listdir(folder)\n                            if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n        self.transform = transforms.Compose([\n            transforms.Resize((256, 256)),\n            transforms.ToTensor(),\n            transforms.Normalize([0.5]*3, [0.5]*3)\n        ])\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img = Image.open(self.image_paths[idx]).convert(\"RGB\")\n        return self.transform(img)\n\n# ======================\n# Paths\n# ======================\nclass_name = \"ANTHRACNOSE256x256\"\ndata_dir = f\"/kaggle/input/gananthracnose256x256/{class_name}\"\noutput_dir = f\"/kaggle/working/DiffusionModel_{class_name.replace(' ', '_')}\"\nos.makedirs(output_dir, exist_ok=True)\n\n# ======================\n# Load VAE\n# ======================\nvae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\").to(\"cuda\").eval()\n\n# ======================\n# Model\n# ======================\ndef get_unet():\n    return UNet2DModel(\n        sample_size=32,   # 256 / 8 (latent size)\n        in_channels=4,\n        out_channels=4,\n        layers_per_block=2,\n        block_out_channels=(128, 256, 512, 512),\n        down_block_types=(\"DownBlock2D\", \"DownBlock2D\", \"DownBlock2D\", \"AttnDownBlock2D\"),\n        up_block_types=(\"AttnUpBlock2D\", \"UpBlock2D\", \"UpBlock2D\", \"UpBlock2D\")\n    )\n\n# ======================\n# Dataloader\n# ======================\ndataset = MangoLeafDataset(data_dir)\ndataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=4)\n\n# ======================\n# Accelerate + Mixed Precision + Multi-GPU\n# ======================\naccelerator = Accelerator(mixed_precision=\"fp16\")  # ‚ö° Mixed precision\nmodel = get_unet()\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\nscheduler = DDPMScheduler(num_train_timesteps=1000)\nmodel, optimizer, dataloader = accelerator.prepare(model, optimizer, dataloader)\n\n# ======================\n# Training Loop\n# ======================\nmodel.train()\nnum_epochs = 100\n\nfor epoch in range(1, num_epochs + 1):\n    print(f\"\\nüîÅ Epoch {epoch}/{num_epochs}\")\n    for batch in tqdm(dataloader, desc=f\"Epoch {epoch}\", disable=not accelerator.is_local_main_process):\n        with torch.no_grad():\n            latents = vae.encode(batch.to(accelerator.device)).latent_dist.sample() * 0.18215\n\n        noise = torch.randn_like(latents)\n        timesteps = torch.randint(0, scheduler.config.num_train_timesteps, (latents.size(0),), device=latents.device).long()\n        noisy_latents = scheduler.add_noise(latents, noise, timesteps)\n\n        noise_pred = model(noisy_latents, timesteps).sample\n        loss = nn.functional.mse_loss(noise_pred, noise)\n\n        optimizer.zero_grad()\n        accelerator.backward(loss)\n        optimizer.step()\n\n    # Save checkpoint\n    if epoch % 10 == 0 and accelerator.is_main_process:\n        ckpt_dir = os.path.join(output_dir, f\"checkpoint_epoch_{epoch}\")\n        model.save_pretrained(ckpt_dir)\n        print(f\"üíæ Saved checkpoint: {ckpt_dir}\")\n\n    # Save sample image\n    if epoch % 20 == 0 and accelerator.is_main_process:\n        model.eval()\n        with torch.no_grad():\n            sample_latents = torch.randn(1, 4, 32, 32).to(accelerator.device)\n            for t in scheduler.timesteps:\n                noise_pred = model(sample_latents, t).sample\n                sample_latents = scheduler.step(noise_pred, t, sample_latents).prev_sample\n\n            decoded_img = vae.decode(sample_latents / 0.18215).sample\n            save_path = os.path.join(output_dir, f\"sample_epoch_{epoch}.png\")\n            save_image(decoded_img, save_path, normalize=True)\n            print(f\"üñºÔ∏è Saved sample image: {save_path}\")\n        model.train()\n\n# ======================\n# Final ZIP\n# ======================\nif accelerator.is_main_process:\n    zip_path = shutil.make_archive(output_dir, 'zip', output_dir)\n    print(f\"‚úÖ Training complete. Model zipped at: {zip_path}\")\n\n   \n       \n      ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T14:03:17.034656Z","iopub.execute_input":"2025-06-12T14:03:17.034939Z"}},"outputs":[{"name":"stderr","text":"2025-06-12 14:03:30.240890: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749737010.479461      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749737010.549967      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/547 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d12875f63cb1488da6fc619d3807a9de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"diffusion_pytorch_model.safetensors:   0%|          | 0.00/335M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"272679a6524a4d39969de6e5622b955d"}},"metadata":{}},{"name":"stdout","text":"\nüîÅ Epoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [12:26<00:00,  1.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nüîÅ Epoch 2/100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [12:49<00:00,  1.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nüîÅ Epoch 3/100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [12:49<00:00,  1.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nüîÅ Epoch 4/100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [13:12<00:00,  1.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nüîÅ Epoch 5/100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [13:20<00:00,  1.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nüîÅ Epoch 6/100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [12:49<00:00,  1.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nüîÅ Epoch 7/100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [12:48<00:00,  1.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nüîÅ Epoch 8/100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [12:48<00:00,  1.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nüîÅ Epoch 9/100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [13:29<00:00,  1.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nüîÅ Epoch 10/100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [13:04<00:00,  1.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"üíæ Saved checkpoint: /kaggle/working/DiffusionModel_ANTHRACNOSE256x256/checkpoint_epoch_10\n\nüîÅ Epoch 11/100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [12:46<00:00,  1.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nüîÅ Epoch 12/100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [12:48<00:00,  1.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nüîÅ Epoch 13/100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [13:03<00:00,  1.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nüîÅ Epoch 14/100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [13:28<00:00,  1.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nüîÅ Epoch 15/100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [12:50<00:00,  1.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nüîÅ Epoch 16/100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [12:49<00:00,  1.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nüîÅ Epoch 17/100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [12:49<00:00,  1.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nüîÅ Epoch 18/100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [13:18<00:00,  1.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nüîÅ Epoch 19/100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1250/1250 [13:29<00:00,  1.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nüîÅ Epoch 20/100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 1158/1250 [12:30<00:59,  1.54it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport zipfile\nfrom torchvision.utils import save_image\nfrom diffusers import DDPMScheduler, AutoencoderKL, UNet2DModel\nfrom tqdm import tqdm\n\n# ==== Configuration ====\nclass_name = \"GALL MILDGE DAMAGE\"\nmodel_dir = f\"/kaggle/working/DiffusionModel_{class_name}\"\noutput_dir = f\"/kaggle/working/Generated_Latent_Images/{class_name}\"\nos.makedirs(output_dir, exist_ok=True)\nnum_images = 1000  # Adjust as needed\n\n# ==== Load VAE ====\nvae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\").to(\"cuda\")\nvae.eval()\n\n# ==== Load trained UNet ====\nunet = UNet2DModel.from_pretrained(model_dir).to(\"cuda\")\nunet.eval()\n\n# ==== Load scheduler ====\nscheduler = DDPMScheduler(num_train_timesteps=1000)\n\n# ==== Generate Images ====\nprint(f\"üé® Generating {num_images} images for class: {class_name}\")\nfor i in tqdm(range(num_images)):\n    latents = torch.randn(1, 4, 64, 64).to(\"cuda\")\n\n    for t in scheduler.timesteps:\n        with torch.no_grad():\n            noise_pred = unet(latents, t).sample\n        latents = scheduler.step(noise_pred, t, latents).prev_sample\n\n    with torch.no_grad():\n        decoded_image = vae.decode(latents).sample\n\n    save_path = os.path.join(output_dir, f\"{class_name}_{i:05}.png\")\n    save_image(decoded_image, save_path, normalize=True)\n\nprint(f\"‚úÖ Image generation complete. Images saved in: {output_dir}\")\n\n# ==== Zip the Generated Images ====\nzip_path = f\"/kaggle/working/{class_name}_Generated.zip\"\nprint(f\"üì¶ Zipping folder {output_dir} ‚Üí {zip_path}\")\nwith zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n    for root, _, files in os.walk(output_dir):\n        for file in files:\n            file_path = os.path.join(root, file)\n            arcname = os.path.relpath(file_path, output_dir)\n            zipf.write(file_path, arcname=arcname)\n\nprint(f\"‚úÖ Zipped dataset saved at: {zip_path}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}